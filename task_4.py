# -*- coding: utf-8 -*-
"""TASK 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1erJuqk4j8W1IuWwlEoK1rJDJ4_WPhYFx
"""

import os
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt #for different plot
from sklearn.preprocessing import OneHotEncoder #for data preprocessing
from sklearn.preprocessing import StandardScaler #for scaling data
from sklearn.model_selection import train_test_split
#for data preprocessing using keras
from keras.preprocessing.image import array_to_img, img_to_array, load_img
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
# keras classes required for building deep CNN model
from keras.models import Sequential, save_model, load_model
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout
from keras.utils import plot_model
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import plotly.figure_factory as ff

import os
import copy
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
import torchvision
from torchvision import models
from sklearn.utils import shuffle
from torchvision import datasets, transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.font_manager
from collections import OrderedDict

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision
import torch
from torch.utils.data import DataLoader, Subset # Import Subset
import torchvision
from torchvision import transforms

# Define your testset here. For example, if you're using a built-in dataset:
testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())

# Define the indices you want to include in the subset
# For example, to include the first 100 samples:
indices = list(range(100))

# Create the subset
subset_data = Subset(testset, indices)

# Create the dataloader
testloader = DataLoader(subset_data, batch_size=64, num_workers=0, shuffle=True)

import os
import zipfile

images=[]
labels=[]

# Open the zip file
with zipfile.ZipFile('/content/drive/MyDrive/archive (2).zip', 'r') as zip_ref:
    # Extract all the contents of zip file in current directory
    zip_ref.extractall('/content/temp_extracted')

# Now loop over the extracted directory
for directory in os.listdir('/content/temp_extracted'):
    for subdir in os.listdir(os.path.join('/content/temp_extracted', directory)):
        for image in os.listdir(os.path.join('/content/temp_extracted', directory, subdir)):
            img_path = os.path.join('/content/temp_extracted', directory, subdir, image)
            images.append(img_path)
            labels.append(subdir)

df = pd.DataFrame({'Image':images,'Label':labels})
df.tail()
# df.info()

df_img = df['Image']
df_label = df['Label']

!pip install keras
import keras
from keras.preprocessing.image import load_img, img_to_array
import numpy as np
import matplotlib.pyplot as plt

nrows=2
ncols = 5
fig,axx = plt.subplots(nrows,ncols, figsize=(12,12))
fig.suptitle("Random Hand Getures Images")

for i in range(nrows):
    for j in range(ncols):
        n = np.random.randint(0, len(df))  # Generate random index within the DataFrame's length
        # Check if df['Image'][n] is a directory, if so, list its contents and select an image
        import os
        if os.path.isdir(df['Image'][n]):
            image_files = [f for f in os.listdir(df['Image'][n]) if f.endswith(('.jpg', '.jpeg', '.png'))]
            if image_files:
                selected_image = os.path.join(df['Image'][n], image_files[0]) # Select the first image in the directory
                img = load_img(selected_image, target_size=(150,150)) #this is a PIL image
            else:
                print(f"No image files found in directory: {df['Image'][n]}")
                continue # Skip to the next iteration
        else:
            img = load_img(df['Image'][n],target_size=(150,150)) # Load image if it's a file

        img = img_to_array(img) #this is a numpy array with shape (3,150,150)
        img = np.expand_dims(img,axis=0) #expand image dimention to (1,3,255,255)
        img /= 255.0 #scale image values between 0,1
        label = df_label[n]
        #changing size from (1,150,150,3) into (150,150,3)
        image = np.squeeze(img)
        axx[i][j].imshow(image)
        axx[i][j].set_title(label)
plt.show()
plt.close()

x_train,x = train_test_split(df, test_size = 0.3)
# train_x, test_x, train_y, test_y = train_test_split(images,labels,test_size=0.2)
x_valid,x_test = train_test_split(x,test_size=0.5)
x_train.info()
x_valid.info()
x_test.info()

datagen = ImageDataGenerator(
        rotation_range=10, # rotation
        width_shift_range=0.2, # horizontal shift
        height_shift_range=0.2, # vertical shift
        zoom_range=0.2, # zoom
        horizontal_flip=True, # horizontal flip
        brightness_range=[0.2,1.2]) # brightness


train_datagen = ImageDataGenerator(rescale=1.0/255,
                                 shear_range=0.2,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
validate_datagen = ImageDataGenerator(rescale=1.0/255)

b_size =1
train_generator = train_datagen.flow_from_dataframe(dataframe=x_train,x_col="Image",y_col='Label',class_mode="categorical",target_size=(150,150),batch_size=b_size,seed=2020, shuffle=True)
valid_generator = validate_datagen.flow_from_dataframe(dataframe=x_valid,x_col="Image",y_col='Label',class_mode="categorical",target_size=(150,150),batch_size=b_size,seed=2020, shuffle=False)

fig,axx = plt.subplots(nrows=1,ncols=4, figsize=(15,15))

for i in range(4):
    #convert into unsigned integers for plotting
    image_batch = next(train_generator)  # Get a batch of images
    if image_batch[0].size > 0:  # Check if the batch is not empty
        image = image_batch[0][0].astype('float64')  # Extract the first image from the batch
        #changing size from (1,150,150,3) into (150,150,3)
        image = np.squeeze(image)
        #plot row pixel data
        axx[i].imshow(image)
        axx[i].axis('off')
    else:
        print(f"Skipping empty batch at index {i}")

model = Sequential()
model.add(Conv2D(32,(3,3), input_shape=(150,150,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(32,(3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

# on top of the model we add 2 fully connected layers to flatten the 3D feature maps into 10 feature victors representing 10 differnet categories of hanf geatures
model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation('softmax'))

model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
             optimizer = 'adam',
             metrics=[tf.keras.metrics.CategoricalAccuracy()])
model.summary()

plot_model(
    model,
    to_file="model.png",
    show_shapes=True,
    show_dtype=True,
    show_layer_names=True
)

# Check if the training generator yields data and its shape
try:
    batch = next(train_generator)
    print("Training generator is working. Batch shape:", batch[0].shape)  # Print shape of the batch
    print("Number of labels in the batch:", len(batch[1]))
except StopIteration:
    print("Training generator is empty. Check your data and generator setup.")

# Check if the validation generator yields data and its shape
try:
    batch = next(valid_generator)
    print("Validation generator is working. Batch shape:", batch[0].shape)  # Print shape of the batch
    print("Number of labels in the batch:", len(batch[1]))
except StopIteration:
    print("Validation generator is empty. Check your data and generator setup.")

# ... (rest of the code remains unchanged)